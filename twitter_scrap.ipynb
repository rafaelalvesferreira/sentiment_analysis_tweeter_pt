{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /private/var/folders/zc/jm1zv2_94qs5k_5xkchcbd200000gn/T/pip-req-build-io6ngqi6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/JustAnotherArchivist/snscrape.git /private/var/folders/zc/jm1zv2_94qs5k_5xkchcbd200000gn/T/pip-req-build-io6ngqi6\n",
      "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit e13033fea0da6e64684513b2fbc489883698f28a\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests[socks] in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from snscrape==0.4.3.20220107.dev63+ge13033f) (2.28.1)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.1-cp39-cp39-macosx_10_15_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from snscrape==0.4.3.20220107.dev63+ge13033f) (4.11.1)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from beautifulsoup4->snscrape==0.4.3.20220107.dev63+ge13033f) (2.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev63+ge13033f) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev63+ge13033f) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev63+ge13033f) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev63+ge13033f) (1.26.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/miniconda3/envs/fast/lib/python3.9/site-packages (from requests[socks]->snscrape==0.4.3.20220107.dev63+ge13033f) (1.7.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for snscrape: filename=snscrape-0.4.3.20220107.dev63+ge13033f-py3-none-any.whl size=67978 sha256=3f25dda871e41296be13820b293b2fa95875ff51361e9a1531f8c04a2c2d4d78\n",
      "  Stored in directory: /private/var/folders/zc/jm1zv2_94qs5k_5xkchcbd200000gn/T/pip-ephem-wheel-cache-cvysxs5w/wheels/1a/ba/e2/39fa3a11802c4a622f2efc8be3f5ff854481051d0b4c95c1fd\n",
      "Successfully built snscrape\n",
      "Installing collected packages: lxml, filelock, snscrape\n",
      "Successfully installed filelock-3.8.0 lxml-4.9.1 snscrape-0.4.3.20220107.dev63+ge13033f\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Rode o código se precisar instalar o snscrape\n",
    "%pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Rode esse código se precisar instalar o Pandas\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapTweets(max_tweets: int, search_string: str):\n",
    "    # Definindo o máximo de tweets\n",
    "    maxTweets = max_tweets\n",
    "\n",
    "    # Criando a lista para salvar os dados\n",
    "    tweets_list = []\n",
    "\n",
    "    # Usando o TwitterSearchScraper para fazer o scrape dos dados data e salvar em uma lista\n",
    "    for i, tweet in enumerate(\n",
    "        sntwitter.TwitterSearchScraper(search_string).get_items()\n",
    "    ):\n",
    "        if i > maxTweets:\n",
    "            break\n",
    "        tweets_list.append(\n",
    "            [\n",
    "                tweet.date,\n",
    "                tweet.replyCount,\n",
    "                tweet.retweetCount,\n",
    "                tweet.likeCount,\n",
    "                tweet.place,\n",
    "                tweet.sourceLabel,\n",
    "                tweet.rawContent,\n",
    "                tweet.user.username,\n",
    "                tweet.user.followersCount,\n",
    "                tweet.user.friendsCount,\n",
    "                tweet.user.created,\n",
    "                tweet.user.verified,\n",
    "                tweet.user.statusesCount,\n",
    "                tweet.user.favouritesCount,\n",
    "                tweet.user.listedCount,\n",
    "                tweet.user.mediaCount,\n",
    "            ]\n",
    "        )\n",
    "    # Criando um dataframe com os dados\n",
    "    tweets_df = pd.DataFrame(\n",
    "        tweets_list,\n",
    "        columns=[\n",
    "            \"DataHora\",\n",
    "            \"ReplyCount\",\n",
    "            \"Retweet Cont\",\n",
    "            \"LikeCount\",\n",
    "            \"Local\",\n",
    "            \"Source\",\n",
    "            \"Texto\",\n",
    "            \"Usuario\",\n",
    "            \"NumSeguidores\",\n",
    "            \"NumAmigos\",\n",
    "            \"UsuarioCriado\",\n",
    "            \"UsuarioVerificado\",\n",
    "            \"StatusCount\",\n",
    "            \"favouritesCount\",\n",
    "            \"listedCount\",\n",
    "            \"mediaCount\",\n",
    "        ],\n",
    "    )\n",
    "    # Adicionando a coluna de data, mês e hora\n",
    "    tweets_df[\"dia\"] = tweets_df.DataHora.dt.day\n",
    "    tweets_df[\"mes\"] = tweets_df.DataHora.dt.month\n",
    "    tweets_df[\"ano\"] = tweets_df.DataHora.dt.year\n",
    "\n",
    "    return tweets_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormatToIRAMUTEQ(tweets: pd.DataFrame):\n",
    "    # Criando um dicionário a partir do dataframe\n",
    "    tweets_dict = tweets.to_dict()\n",
    "\n",
    "    with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as corpus:\n",
    "        for i in range(len(tweets_dict[\"DataHora\"])):\n",
    "            corpus.write(\"**** \")\n",
    "            corpus.write(f'*ano_{tweets_dict[\"ano\"][i]} ')\n",
    "            corpus.write(f'*mes_{tweets_dict[\"mes\"][i]} ')\n",
    "            corpus.write(f'*dia_{tweets_dict[\"dia\"][i]} ')\n",
    "            corpus.write(f'*usuario_{tweets_dict[\"Usuario\"][i]} ')\n",
    "            corpus.write(f'*fonte_{tweets_dict[\"Source\"][1].replace(\" \", \"_\")}\\n')\n",
    "            corpus.write(tweets_dict[\"Texto\"][i].replace(\"\\n\", \" \").replace(\"*\", \"\"))\n",
    "            corpus.write(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Na string de busca você pode usar ou omitir os marcadores from e until\n",
    "# Exemplo:  'rockinrio from:anitta until:2020-07-31'\n",
    "\n",
    "tweets = ScrapTweets(1000, \"COVID vacina cloroquina\")\n",
    "\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar dataframe para CSV\n",
    "tweets.to_csv(\"text-query-tweets.csv\", sep=\",\", index=False)\n",
    "\n",
    "# Criar corpus para IRAMUTEQ\n",
    "FormatToIRAMUTEQ(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0067ede7a6e73994e39246acd9e1f0672d1160e6473dfe982c0ca5351566aed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
