{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Naive Bayes model on the Training set\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import datetime as dt\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Data Preprocessing and Feature Engineering\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import spacy\n",
    "\n",
    "sns.set()  # use seaborn plotting style\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo o Scraps dos tweets\n",
    "\n",
    "def ScrapTweets(max_tweets: int, search_string: str):\n",
    "    # Definindo o máximo de tweets\n",
    "    maxTweets = max_tweets\n",
    "\n",
    "    # Criando a lista para salvar os dados\n",
    "    tweets_list = []\n",
    "\n",
    "    # Usando o TwitterSearchScraper para fazer o scrape dos dados data e salvar em uma lista\n",
    "    for i, tweet in enumerate(\n",
    "        sntwitter.TwitterSearchScraper(search_string).get_items()\n",
    "    ):\n",
    "        if i > maxTweets:\n",
    "            break\n",
    "        tweets_list.append(\n",
    "            [\n",
    "                tweet.id,\n",
    "                tweet.date,\n",
    "                tweet.rawContent,\n",
    "                tweet.lang,\n",
    "            ]\n",
    "        )\n",
    "    # Criando um dataframe com os dados\n",
    "    tweets_df = pd.DataFrame(\n",
    "        tweets_list,\n",
    "        columns=[\n",
    "            \"TweetId\",\n",
    "            \"DataHora\",\n",
    "            \"Texto\",\n",
    "            \"Idioma\",\n",
    "        ],\n",
    "    )\n",
    "    # Adicionando a coluna de data, mês e hora\n",
    "    tweets_df[\"dia\"] = tweets_df.DataHora.dt.day\n",
    "    tweets_df[\"mes\"] = tweets_df.DataHora.dt.month\n",
    "    tweets_df[\"ano\"] = tweets_df.DataHora.dt.year\n",
    "\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_feliz = ScrapTweets(10000, \"#feliz\")\n",
    "tweets_feliz[\"emocao\"] = \"feliz\"\n",
    "\n",
    "tweets_amor = ScrapTweets(10000, \"#amo\")\n",
    "tweets_amor[\"emocao\"] = \"amor\"\n",
    "\n",
    "tweets_paixao = ScrapTweets(10000, \"paixao\")\n",
    "tweets_paixao[\"emocao\"] = \"paixao\"\n",
    "\n",
    "tweets_alegria = ScrapTweets(10000, \"#alegria\") #\n",
    "tweets_alegria[\"emocao\"] = \"alegria\"\n",
    "\n",
    "tweets_otimismo = ScrapTweets(10000, \"#otimismo\") #\n",
    "tweets_otimismo[\"emocao\"] = \"otimismo\"\n",
    "\n",
    "tweets_confianca = ScrapTweets(10000, \"#confiança\") #\n",
    "tweets_confianca[\"emocao\"] = \"confiança\"\n",
    "\n",
    "tweets_medo = ScrapTweets(10000, \"#medo\") #\n",
    "tweets_medo[\"emocao\"] = \"medo\"\n",
    "\n",
    "tweets_nojo = ScrapTweets(10000, \"#nojo\") #\n",
    "tweets_nojo[\"emocao\"] = \"nojo\"\n",
    "\n",
    "tweets_triste = ScrapTweets(10000, \"#triste\") #\n",
    "tweets_triste[\"emocao\"] = \"triste\"\n",
    "\n",
    "tweets_chateado = ScrapTweets(5000, \"#chateado\")\n",
    "tweets_chateado[\"emocao\"] = \"chateado\"\n",
    "\n",
    "tweets_chateada = ScrapTweets(5000, \"#chateada\")\n",
    "tweets_chateada[\"emocao\"] = \"chateado\"\n",
    "\n",
    "tweets_raiva = ScrapTweets(10000, \"#raiva\") #\n",
    "tweets_raiva[\"emocao\"] = \"raiva\"\n",
    "\n",
    "tweets_vergonha = ScrapTweets(10000, \"#vergonha\")\n",
    "tweets_vergonha[\"emocao\"] = \"vergonha\"\n",
    "\n",
    "\n",
    "tweets = pd.concat(\n",
    "    [\n",
    "        tweets_feliz,\n",
    "        tweets_amor,\n",
    "        tweets_paixao,\n",
    "        tweets_alegria,\n",
    "        tweets_otimismo,\n",
    "        tweets_confianca,\n",
    "        tweets_medo,\n",
    "        tweets_nojo,\n",
    "        tweets_triste,\n",
    "        tweets_chateado,\n",
    "        tweets_chateada,\n",
    "        tweets_raiva,\n",
    "        tweets_vergonha\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Exportar dataframe para CSV\n",
    "tweets.to_csv(\"text-query-tweets.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # importing the dataset\n",
    "# tweets = pd.read_csv(\"text-query-tweets.csv\", sep=\",\")\n",
    "# tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>emocao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@dizaoleao Estou de fora, #triste</td>\n",
       "      <td>triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Promessa não deu certo, partiu pra ameaça #tri...</td>\n",
       "      <td>triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Triste ver minha cidade dividida por causa do...</td>\n",
       "      <td>triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Hugo_aguiar09 tô precisando, to meio #triste</td>\n",
       "      <td>triste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>É engraçado quando você namora por muito tempo...</td>\n",
       "      <td>triste</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto  emocao\n",
       "0                  @dizaoleao Estou de fora, #triste  triste\n",
       "1  Promessa não deu certo, partiu pra ameaça #tri...  triste\n",
       "2  #Triste ver minha cidade dividida por causa do...  triste\n",
       "3      @Hugo_aguiar09 tô precisando, to meio #triste  triste\n",
       "4  É engraçado quando você namora por muito tempo...  triste"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removendo duplicatas\n",
    "tweets.drop_duplicates(subset=\"TweetId\", keep=False, inplace=True)\n",
    "\n",
    "# Filtrando o idimoa para português\n",
    "tweets = tweets[tweets[\"Idioma\"] == \"pt\"]\n",
    "\n",
    "# Removendo colunas desnecessárias\n",
    "tweets.drop(\n",
    "    [\"TweetId\", \"DataHora\", \"Idioma\", \"dia\", \"mes\", \"ano\"], axis=1, inplace=True\n",
    ")\n",
    "\n",
    "tweets.reset_index(drop=True, inplace=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment(sentiment):\n",
    "    if sentiment in [\"feliz\", \"amor\", \"paixao\", \"alegria\", \"otimismo\", \"confiança\"]:\n",
    "        return 1\n",
    "    elif sentiment in [\"medo\", \"nojo\", \"triste\", \"chateado\", \"raiva\", \"vergonha\"]:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "tweets['sentimento'] = tweets['emocao'].apply(define_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funções de tratamento de dados\n",
    "def remove_usernames(tweet):\n",
    "        tweet = re.sub(r'@\\S+', '', tweet)\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)\n",
    "        return tweet\n",
    "\n",
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "def no_stopwords(tweet):\n",
    "    clean_mess = [word for word in tweet.split() if word.lower() not in stopwords.words('portuguese')]\n",
    "    clean_mess = ' '.join(clean_mess)\n",
    "    return clean_mess\n",
    "\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        doc = nlp(tweet_list)\n",
    "        return ' '.join([token.lemma_ for token in doc if token.pos_ == 'VERB' or token.pos_ == 'NOUN' or token.pos_ == 'ADJ'])\n",
    "\n",
    "# funçao analyser\n",
    "def text_processing(tweet):\n",
    "    new_tweet = remove_usernames(tweet)\n",
    "    no_punc_tweet = form_sentence(new_tweet)\n",
    "    no_stopwords_tweet = no_stopwords(no_punc_tweet)\n",
    "    return normalization(no_stopwords_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>emocao</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>promessa dar certo partir ameaça triste revolt...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Triste ver cidade dividir causa ego político c...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precisar triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>engraçar namorar tempo ficar solteiro pessoa a...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>triste panna cota Vim salivar</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cara triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>triste querer dizer coisa saber heh</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inércia intelectual maioria povo gritante para...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>triste desilusão depressão dor decepção transt...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Acabei descobrir preciso novo amigo consigo vi...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chamar proprio podre triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Passei ano pensar tentar escrever série descub...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>apagar triste descepção</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>único coisa querer beber birita escutar barzin...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fingir ver colega falar pq prefiro acompanhar ...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Faltou moreno lado triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>caminhão acabar graça tirar foto placa saber t...</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bom dia triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>peninhar triste</td>\n",
       "      <td>triste</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Texto  emocao  sentimento\n",
       "0                                              triste  triste           0\n",
       "1   promessa dar certo partir ameaça triste revolt...  triste           0\n",
       "2   Triste ver cidade dividir causa ego político c...  triste           0\n",
       "3                                     precisar triste  triste           0\n",
       "4   engraçar namorar tempo ficar solteiro pessoa a...  triste           0\n",
       "5                       triste panna cota Vim salivar  triste           0\n",
       "6                                         cara triste  triste           0\n",
       "7                 triste querer dizer coisa saber heh  triste           0\n",
       "8   inércia intelectual maioria povo gritante para...  triste           0\n",
       "9   triste desilusão depressão dor decepção transt...  triste           0\n",
       "10  Acabei descobrir preciso novo amigo consigo vi...  triste           0\n",
       "11                        chamar proprio podre triste  triste           0\n",
       "12  Passei ano pensar tentar escrever série descub...  triste           0\n",
       "13                            apagar triste descepção  triste           0\n",
       "14  único coisa querer beber birita escutar barzin...  triste           0\n",
       "15  Fingir ver colega falar pq prefiro acompanhar ...  triste           0\n",
       "16                          Faltou moreno lado triste  triste           0\n",
       "17  caminhão acabar graça tirar foto placa saber t...  triste           0\n",
       "18                                     bom dia triste  triste           0\n",
       "19                                    peninhar triste  triste           0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Texto'] = tweets['Texto'].apply(text_processing)\n",
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 unique classes\n",
      "We have 128588 training samples\n",
      "We have 32147 test samples\n"
     ]
    }
   ],
   "source": [
    "# Buscando categorias\n",
    "text_categories = tweets[\"sentimento\"].unique()\n",
    "\n",
    "# Separando os dados de treino e teste\n",
    "X = tweets[\"Texto\"]\n",
    "y = tweets[\"sentimento\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"We have {} unique classes\".format(len(text_categories)))\n",
    "print(\"We have {} training samples\".format(len(X_train)))\n",
    "print(\"We have {} test samples\".format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    142307\n",
      "1     18428\n",
      "Name: sentimento, dtype: int64\n",
      "Training target statistics: Counter({0: 14709, 1: 14709})\n",
      "Testing target statistics: Counter({0: 28428, 1: 3719})\n"
     ]
    }
   ],
   "source": [
    "print(tweets['sentimento'].value_counts())\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "X_train_res, y_train_res = under_sampler.fit_resample(X_train.values.reshape(-1, 1), y_train)\n",
    "X_train_res = X_train_res.reshape(-1)\n",
    "print(f\"Training target statistics: {Counter(y_train_res)}\")\n",
    "\n",
    "X_test_res, y_test_res = under_sampler.fit_resample(X_test.values.reshape(-1, 1), y_test)\n",
    "X_test_res = X_test_res.reshape(-1)\n",
    "print(f\"Training target statistics: {Counter(y_test_res)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     27694\n",
      "           1       0.86      0.72      0.78      4453\n",
      "\n",
      "    accuracy                           0.95     32147\n",
      "   macro avg       0.91      0.85      0.88     32147\n",
      "weighted avg       0.94      0.95      0.94     32147\n",
      "\n",
      "[[27182   512]\n",
      " [ 1246  3207]]\n",
      "0.9453137151211621\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(analyzer='word')),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# tuned_parameters = {\n",
    "#     'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "#     'clf__alpha': [1, 1e-1, 1e-2]\n",
    "# }\n",
    "\n",
    "# clf = GridSearchCV(text_clf, tuned_parameters, cv=10, scoring='accuracy')\n",
    "# clf.fit(X_train, y_train)\n",
    "text_clf.fit(X_train_res, y_train_res)\n",
    "predictions = text_clf.predict(X_test_res)\n",
    "print(classification_report(predictions, y_test_res))\n",
    "print(confusion_matrix(predictions, y_test_res))\n",
    "print(accuracy_score(predictions, y_test_res))\n",
    "\n",
    "# print(classification_report(y_test, clf.predict(X_test), digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_prediction (text):\n",
    "#     return text_clf.predict([text_processing(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(text_clf, \"tweeter_multinomialNB_pt.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fast')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0067ede7a6e73994e39246acd9e1f0672d1160e6473dfe982c0ca5351566aed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
